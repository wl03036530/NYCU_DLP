from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random
import time
import math
import torch
import torch.nn as nn
from torch import optim
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
from torch.nn.modules.loss import CrossEntropyLoss
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
import os
import CVAE
import dataloader
from torch.utils.data import DataLoader
import datetime

test_mode = False
################################
#Example inputs of compute_bleu
################################
#The target word
#reference = 'accessed'
#The word generated by your model
#output = 'access'

#compute BLEU-4 score
def compute_bleu(output, reference):
    cc = SmoothingFunction()
    if len(reference) == 3:
        weights = (0.33,0.33,0.33)
    else:
        weights = (0.25,0.25,0.25,0.25)
    return sentence_bleu([reference], output,weights=weights,smoothing_function=cc.method1)


"""============================================================================
example input of Gaussian_score

words = [['consult', 'consults', 'consulting', 'consulted'],
['plead', 'pleads', 'pleading', 'pleaded'],
['explain', 'explains', 'explaining', 'explained'],
['amuse', 'amuses', 'amusing', 'amused'], ....]

the order should be : simple present, third person, present progressive, past
============================================================================"""

def Gaussian_score(words):
    words_list = []
    score = 0
    yourpath = './train.txt' #should be your directory of train.txt
    with open(yourpath,'r') as fp:
        for line in fp:
            word = line.split(' ')
            word[3] = word[3].strip('\n')
            words_list.extend([word])
        for t in words:
            for i in words_list:
                if t == i:
                    score += 1
    return score/len(words)
        
def savePlt_png(epoch_size, KLD, CrossEntropy, KLD_weight, Teacher_ratio, Guassian, BLEU4):
    x_epoch = range(epoch_size)

    fig, ax1 = plt.subplots()
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    lns1 = ax1.plot(x_epoch, CrossEntropy, linestyle='-', linewidth=1, color='orange', label="CrossEntropy")
    lns2 = ax1.plot(x_epoch, KLD, linestyle='-', linewidth=1, color='royalblue', label="KLD")

    ax2 = ax1.twinx() 
    ax2.set_ylabel("score / weight") 
    lns3 = ax2.plot(x_epoch, BLEU4, linestyle='None', marker='o', markersize=2, color='darkgreen', label="BLEU4-score")
    lns4 = ax2.plot(x_epoch, Guassian, linestyle='None', marker='o', markersize=2, color='darkgoldenrod', label="Guassian-score")
    lns5 = ax2.plot(x_epoch, Teacher_ratio, linestyle='--', linewidth=1, color='blueviolet', label="Teacher ratio")
    lns6 = ax2.plot(x_epoch, KLD_weight, linestyle='--', linewidth=1, color='red', label="KLD_weight")
    
    
    lns = lns1+lns2+lns3+lns4+lns5+lns6
    labs = [l.get_label() for l in lns]
    ax1.legend(lns, labs, loc='center left')
    plt.title("Training loss, ratio and scores")

    plt.savefig('training_result.png')

####-------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_dataset = dataloader.wordsDataset()
test_dataset = dataloader.wordsDataset(False)

sos_token = train_dataset.chardict.word2index['SOS']
eos_token = train_dataset.chardict.word2index['EOS']
#----------Hyper Parameters----------#
hidden_size = 256
latent_size = 32
condition_size = 8
#The number of vocabulary
word_size = train_dataset.chardict.n_words
num_condition = len(train_dataset.tenses)


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

def teacher_forcing_ratio(epoch):
    # from 1.0 to 0.0
    slope = 0.01
    level = 10
    w = 1.0 - (slope * (epoch//level))
    if w <= 0.0:
        w = 0.0
    
    return w

def KLD_weight_annealing(epoch):
    block = 10
    slope = 0.01

    if(epoch < 20):
        return 0

    w = slope * ((epoch-20) % block)

    if w > 1.0:
        w = 1.0
    
    return w

def trainIters(encoder, decoder, tfr, kld_w, learning_rate=0.01):
    encoder.train()
    decoder.train()
    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()

    CE_loss = 0
    KLD_loss = 0
    total_loss = 0
    train_dataset = dataloader.wordsDataset()
    train_dataset = DataLoader(train_dataset, shuffle=True)
    # for idx in range(len(train_dataset)):
    for idx, (input, c) in enumerate(train_dataset):
        
        print("\r{:>6d}/{:>6d}".format(idx, len(train_dataset)), end="")
        # data = train_dataset[idx]
        # input, c = data
        input = input.squeeze()
        c = c.item()

        #input has no SOS and EOS
        z, m, logvar = encoder(input[1:-1].to(device), encoder.initHidden().to(device), c)

        # decide teacher forcing
        use_teacher_forcing = True if random.random() < tfr else False

        #output
        loss = 0
        x = torch.LongTensor([sos_token]).to(device)
        z = z.view(1,1,-1)
        hidden, c_0 = decoder.initHidden(z.to(device), encoder.condition(c))

        for i in range(input.size(0)-1): #without SOS EOS
            x = x.detach()
            output, hidden, c_0 = decoder(x.to(device), hidden, c_0)
            output_onehot = torch.max(torch.softmax(output, dim=1), 1)[1]
            x = output_onehot

            if x.item() == eos_token and not use_teacher_forcing:
                break
            if use_teacher_forcing:
                x = input[i+1]

            #loss
            loss += criterion(output, input[i+1].to(device).view(-1))
        
        encoder_optimizer.zero_grad()
        decoder_optimizer.zero_grad()
        # loss /= 10

        CE_loss += loss
        KLD_loss += CVAE.KL_loss(m, logvar)
        total_loss = loss + (CVAE.KL_loss(m, logvar) * kld_w)
        # total_loss += loss + (CVAE.KL_loss(m, logvar) * kld_w)

        # if(idx % 4 == 0):
        #     encoder_optimizer.zero_grad()
        #     decoder_optimizer.zero_grad()
        #     total_loss.backward()
        #     encoder_optimizer.step()
        #     decoder_optimizer.step()
        #     total_loss = 0

        if(test_mode):
            break

        total_loss.backward()

        encoder_optimizer.step()
        decoder_optimizer.step()

    CE_loss /= len(train_dataset)
    KLD_loss /= len(train_dataset)
    print('\rTrain set: CE loss: {:7.4f}, KLD loss: {:7.4f}'.format(CE_loss.item(), KLD_loss.item()))
    return CE_loss.item(), KLD_loss.item()

def eval(encoder, decoder):
    encoder.eval()
    decoder.eval()
    BLEU4_score = 0
    test_dataset = dataloader.wordsDataset(False)
    with torch.no_grad():
        for idx in range(len(test_dataset)):
            data = test_dataset[idx]
            input, input_condition, targets, target_condition = data
            #input has no SOS and EOS
            z, m, logvar = encoder(input[1:-1].to(device), encoder.initHidden().to(device), input_condition)
            #output
            x = torch.LongTensor([sos_token]).to(device)
            z = z.view(1,1,-1)
            hidden, c_0 = decoder.initHidden(z.to(device), encoder.condition(target_condition))
            word = []
            for i in range(targets.size(0)-1): #without SOS EOS
                x = x.detach()
                output, hidden, c_0 = decoder(x.to(device), hidden, c_0)
                output_onehot = torch.max(torch.softmax(output, dim=1), 1)[1]
                x = output_onehot

                if x.item() == eos_token:
                    break

                word.append(x)
            # convert type
            #word = torch.cat(word, dim=0)
            inputs_str = train_dataset.chardict.stringFromLongtensor(input, check_end=True)
            targets_str = train_dataset.chardict.stringFromLongtensor(targets, check_end=True)
            outputs_str = train_dataset.chardict.stringFromLongtensor(word, check_end=True)

            print(inputs_str, '->', targets_str,':',outputs_str)
            BLEU4_score += compute_bleu(outputs_str, targets_str)

        # generation
        print("Words generation: ", end="\n")
        words = []
        for i in range(100):
            word = []
            latent = torch.randn([1, 1, latent_size]).to(device)
            for j in range(4):
                hidden, c_0 = decoder.initHidden(latent, encoder.condition(j))
                x = torch.LongTensor([sos_token]).to(device)
                seq = []
                for idx in range(16):
                    x = x.detach()
                    output, hidden, c_0 = decoder(x.to(device), hidden, c_0)
                    output_onehot = torch.max(torch.softmax(output, dim=1), 1)[1]
                    x = output_onehot

                    if x.item() == eos_token:
                        break

                    seq.append(x)
                # convert type
                #seq = torch.cat(seq, dim=0)
                outputs_str = train_dataset.chardict.stringFromLongtensor(seq, check_end=True)
                word.append(outputs_str)
            words.append(word)
        for w in words:
            print(w)

    BLEU4_score /= len(test_dataset)
    gaussian_score = Gaussian_score(words)
    print('\rTest  set: BLEU-4 score: {:7.4f}, Gaussian score: {:7.4f}'.format(BLEU4_score, gaussian_score))
    return BLEU4_score, gaussian_score
        


def train():
    encoder = CVAE.EncoderRNN(word_size, hidden_size, latent_size, num_condition, condition_size).to(device)
    decoder = CVAE.DecoderRNN(word_size, hidden_size, latent_size, condition_size).to(device)
    epoch_size = 100
    lr = 0.007

    KLD = []
    CrossEntropy = []
    KLD_weight = []
    Teacher_ratio = []
    Guassian = []
    BLEU4 = []

    for epoch in range(epoch_size):        
        tfr = teacher_forcing_ratio(epoch)
        kld_w = KLD_weight_annealing(epoch)

        now = datetime.datetime.now().strftime("%H:%M:%S")
        print('\nEpoch: {:>7d}, Time: {}'.format(epoch, now))
        CE_loss, KLD_loss = trainIters(encoder, decoder, tfr, kld_w, lr)
        BLEU4_score, guassian_score = eval(encoder, decoder)

        
        KLD.append(KLD_loss)
        CrossEntropy.append(CE_loss)
        KLD_weight.append(kld_w)
        Teacher_ratio.append(tfr)
        Guassian.append(guassian_score)
        BLEU4.append(BLEU4_score)

        encoder_Path = os.path.join(os.getcwd(), 'encoder')
        if not os.path.exists(encoder_Path):
            os.makedirs(encoder_Path)

        decoder_Path = os.path.join(os.getcwd(), 'decoder')
        if not os.path.exists(decoder_Path):
            os.makedirs(decoder_Path)

        if BLEU4_score >= 0.4 and guassian_score >= 0.2:
            torch.save(encoder.state_dict(), os.path.join(encoder_Path, 'encoder_'+str(epoch)+'_'+str(int(BLEU4_score*100))+'_'+str(int(guassian_score*100))+'.pkl'))
            torch.save(decoder.state_dict(), os.path.join(decoder_Path, 'decoder_'+str(epoch)+'_'+str(int(BLEU4_score*100))+'_'+str(int(guassian_score*100))+'.pkl'))

    savePlt_png(epoch_size, KLD, CrossEntropy, KLD_weight, Teacher_ratio, Guassian, BLEU4)
        
def test():
    encoder = CVAE.EncoderRNN(word_size, hidden_size, latent_size, num_condition, condition_size).to(device)
    decoder = CVAE.DecoderRNN(word_size, hidden_size, latent_size, condition_size).to(device)
    epoch_size = 100

    encoder_Path = os.path.join(os.getcwd(), 'encoder')
    decoder_Path = os.path.join(os.getcwd(), 'decoder')

    encoder.load_state_dict(torch.load(os.path.join(encoder_Path, "encoder_98_92_32.pkl")))
    decoder.load_state_dict(torch.load(os.path.join(decoder_Path, "decoder_98_92_32.pkl")))

    _, _ = eval(encoder, decoder)

if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # train()
    test_mode = True
    test()